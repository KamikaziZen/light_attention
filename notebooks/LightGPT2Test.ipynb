{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a9a9d95-d177-4414-9097-4faf97666a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee9efd87-b338-4779-9e32-f0c26f8a0ddb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-28 13:37:15.743104: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Config, GPT2Tokenizer, GPT2LMHeadModel, GPT2Model\n",
    "import transformers\n",
    "import torch\n",
    "from torch import nn\n",
    "import copy\n",
    "import time\n",
    "import tqdm\n",
    "import gc\n",
    "\n",
    "from light_attention.models.gpt2 import LightGPT2LMHeadModel, LightGPT2Model\n",
    "from light_attention.profile import estimate_layer_memory, mem_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c161eee-1a9c-4782-9dff-da4d1a92caf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1.10.0+cu111', '4.20.1', device(type='cuda'))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__, transformers.__version__, torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3803db79-bbf3-4c85-9cc6-31302f971f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 15835"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53d4e32-683a-4724-8b54-30cdf8b11717",
   "metadata": {},
   "source": [
    "# Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bdadb0-189c-40ca-a14b-1a696d025feb",
   "metadata": {},
   "source": [
    "### GPT2-small"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b147fbcc-ff78-418e-b591-d82528aa740f",
   "metadata": {},
   "source": [
    "### Vanilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7219ec92-4e2d-4f28-b916-22d276b5e55e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Before placing the model on GPU\n",
      "MA 0.0312 MB         Max_MA 0.0312 MB         CA 2.0 MB         Max_CA 2.0 MB \n",
      "\n",
      "After placing the model on GPU:\n",
      "MA 487.5 MB         Max_MA 487.5 MB         CA 542.0 MB         Max_CA 542.0 MB \n",
      "\n",
      "Params (empirical) 487.4688 MB\n",
      "\n",
      "After input batch generation, before forward pass:\n",
      "MA 487.5 MB         Max_MA 487.5 MB         CA 542.0 MB         Max_CA 542.0 MB \n",
      "\n",
      "After backward:\n",
      "MA 12119.2896 MB         Max_MA 12119.2896 MB         CA 12654.0 MB         Max_CA 12654.0 MB \n",
      "\n",
      "Activations (empirical) 11631.7896 MB\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(RANDOM_SEED)\n",
    "configuration = GPT2Config(n_head=12, n_layer=12, n_positions=1024, n_embd=768)\n",
    "model = GPT2Model(configuration)\n",
    "b = 4\n",
    "seq = configuration.n_positions\n",
    "emb = configuration.n_embd\n",
    "x = torch.randint(0, configuration.vocab_size, size=(b,seq), device='cuda')\n",
    "estimate_layer_memory(copy.deepcopy(model), x, device='cuda', input_shape=None)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb9686f-dfeb-4aac-8ffe-6809b9c130d0",
   "metadata": {},
   "source": [
    "### Light"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "daf553ea-0e80-45e0-85e1-b3c52058ffac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Before placing the model on GPU\n",
      "MA 0.0312 MB         Max_MA 0.0312 MB         CA 2.0 MB         Max_CA 2.0 MB \n",
      "\n",
      "After placing the model on GPU:\n",
      "MA 487.5 MB         Max_MA 487.5 MB         CA 542.0 MB         Max_CA 542.0 MB \n",
      "\n",
      "Params (empirical) 487.4688 MB\n",
      "\n",
      "After input batch generation, before forward pass:\n",
      "MA 487.5 MB         Max_MA 487.5 MB         CA 542.0 MB         Max_CA 542.0 MB \n",
      "\n",
      "After backward:\n",
      "MA 7799.2896 MB         Max_MA 7799.2896 MB         CA 7902.0 MB         Max_CA 7902.0 MB \n",
      "\n",
      "Activations (empirical) 7311.7896 MB\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(RANDOM_SEED)\n",
    "configuration = GPT2Config(n_head=12, n_layer=12, n_positions=1024, n_embd=768)\n",
    "configuration.use_dropmatmul = True\n",
    "configuration.use_lightsoftmax = True\n",
    "model = LightGPT2Model(configuration)\n",
    "b = 4\n",
    "seq = configuration.n_positions\n",
    "x = torch.randint(0, configuration.vocab_size, size=(b,seq), device='cuda')\n",
    "estimate_layer_memory(copy.deepcopy(model), x, device='cuda', input_shape=None)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4025a64d-388d-441a-98a0-1b897789d723",
   "metadata": {},
   "source": [
    "### GPT2-medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6a95361a-70d3-494a-aa14-a44d971bb211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Before placing the model on GPU\n",
      "MA 0.0317 MB         Max_MA 0.0317 MB         CA 2.0 MB         Max_CA 2.0 MB \n",
      "\n",
      "After placing the model on GPU:\n",
      "MA 1377.5864 MB         Max_MA 1377.5864 MB         CA 1380.0 MB         Max_CA 1380.0 MB \n",
      "\n",
      "Params (empirical) 1377.5547 MB\n",
      "\n",
      "Params (analytical, torch) 1353.543 MB\n",
      "\n",
      "After input batch generation, before forward pass:\n",
      "MA 1377.5864 MB         Max_MA 1377.5864 MB         CA 1380.0 MB         Max_CA 1380.0 MB \n",
      "\n",
      "After backward:\n",
      "MA 32351.126 MB         Max_MA 32351.126 MB         CA 33558.0 MB         Max_CA 33558.0 MB \n",
      "\n",
      "\n",
      "Activations (analytical, torchviz) 32485.571 MB\n",
      "Activations (empirical) 30973.5396 MB\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(RANDOM_SEED)\n",
    "configuration = GPT2Config(n_head=16, n_layer=24, n_positions=1024, n_embd=1024)\n",
    "model = GPT2Model(configuration)\n",
    "b = 4\n",
    "seq = configuration.n_positions\n",
    "x = torch.randint(0, configuration.vocab_size, size=(b,seq), device='cuda')\n",
    "estimate_layer_memory(copy.deepcopy(model), x, device='cuda', input_shape=None)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8651c24e-b4cb-4f2c-9ff8-ee43bcaa3ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Before placing the model on GPU\n",
      "MA 0.0317 MB         Max_MA 0.0317 MB         CA 2.0 MB         Max_CA 2.0 MB \n",
      "\n",
      "After placing the model on GPU:\n",
      "MA 1377.5864 MB         Max_MA 1377.5864 MB         CA 1380.0 MB         Max_CA 1380.0 MB \n",
      "\n",
      "Params (empirical) 1377.5547 MB\n",
      "\n",
      "Params (analytical, torch) 1353.543 MB\n",
      "\n",
      "After input batch generation, before forward pass:\n",
      "MA 1377.5864 MB         Max_MA 1377.5864 MB         CA 1380.0 MB         Max_CA 1380.0 MB \n",
      "\n",
      "After backward:\n",
      "MA 20831.126 MB         Max_MA 20831.126 MB         CA 20870.0 MB         Max_CA 20870.0 MB \n",
      "\n",
      "\n",
      "Activations (analytical, torchviz) 20197.5711 MB\n",
      "Activations (empirical) 19453.5396 MB\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(RANDOM_SEED)\n",
    "configuration = GPT2Config(n_head=16, n_layer=24, n_positions=1024, n_embd=1024)\n",
    "configuration.use_dropmatmul = True\n",
    "configuration.use_lightsoftmax = True\n",
    "model = LightGPT2Model(configuration)\n",
    "b = 4\n",
    "seq = configuration.n_positions\n",
    "x = torch.randint(0, configuration.vocab_size, size=(b,seq), device='cuda')\n",
    "estimate_layer_memory(copy.deepcopy(model), x, device='cuda', input_shape=None)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b0d554-5610-4402-ac07-359c9243d767",
   "metadata": {},
   "source": [
    "### GPT2-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c7a7b630-2b42-424d-82c4-f688c8aa71e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Before placing the model on GPU\n",
      "MA 0.0317 MB         Max_MA 0.0317 MB         CA 2.0 MB         Max_CA 2.0 MB \n",
      "\n",
      "After placing the model on GPU:\n",
      "MA 3061.3442 MB         Max_MA 3061.3442 MB         CA 3138.0 MB         Max_CA 3138.0 MB \n",
      "\n",
      "Params (empirical) 3061.3125 MB\n",
      "\n",
      "Params (analytical, torch) 2952.6904 MB\n",
      "\n",
      "After input batch generation, before forward pass:\n",
      "MA 3061.3442 MB         Max_MA 3061.3442 MB         CA 3138.0 MB         Max_CA 3138.0 MB \n",
      "\n",
      "After backward:\n",
      "MA 61104.6338 MB         Max_MA 61104.6338 MB         CA 63376.0 MB         Max_CA 63376.0 MB \n",
      "\n",
      "\n",
      "Activations (analytical, torchviz) 60887.3214 MB\n",
      "Activations (empirical) 58043.2896 MB\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(RANDOM_SEED)\n",
    "configuration = GPT2Config(n_head=20, n_layer=36, n_positions=1024, n_embd=1280)\n",
    "model = GPT2Model(configuration)\n",
    "b = 4\n",
    "seq = configuration.n_positions\n",
    "x = torch.randint(0, configuration.vocab_size, size=(b,seq), device='cuda')\n",
    "estimate_layer_memory(copy.deepcopy(model), x, device='cuda', input_shape=None)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f142ba07-28ae-4834-9ac8-55a2933d70a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Before placing the model on GPU\n",
      "MA 0.0317 MB         Max_MA 0.0317 MB         CA 2.0 MB         Max_CA 2.0 MB \n",
      "\n",
      "After placing the model on GPU:\n",
      "MA 3061.3442 MB         Max_MA 3061.3442 MB         CA 3138.0 MB         Max_CA 3138.0 MB \n",
      "\n",
      "Params (empirical) 3061.3125 MB\n",
      "\n",
      "Params (analytical, torch) 2952.6904 MB\n",
      "\n",
      "After input batch generation, before forward pass:\n",
      "MA 3061.3442 MB         Max_MA 3061.3442 MB         CA 3138.0 MB         Max_CA 3138.0 MB \n",
      "\n",
      "After backward:\n",
      "MA 39504.6338 MB         Max_MA 39504.6338 MB         CA 39616.0 MB         Max_CA 39616.0 MB \n",
      "\n",
      "\n",
      "Activations (analytical, torchviz) 37847.3215 MB\n",
      "Activations (empirical) 36443.2896 MB\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(RANDOM_SEED)\n",
    "configuration = GPT2Config(n_head=20, n_layer=36, n_positions=1024, n_embd=1280)\n",
    "configuration.use_dropmatmul = True\n",
    "configuration.use_lightsoftmax = True\n",
    "model = LightGPT2Model(configuration)\n",
    "b = 4\n",
    "seq = configuration.n_positions\n",
    "x = torch.randint(0, configuration.vocab_size, size=(b,seq), device='cuda')\n",
    "estimate_layer_memory(copy.deepcopy(model), x, device='cuda', input_shape=None)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bdf238-22bd-48f6-9011-70b51d1120f1",
   "metadata": {},
   "source": [
    "## GPT2-xl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "520d3f18-dc2d-4ede-88df-14ea023d16b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Before placing the model on GPU\n",
      "MA 0.0156 MB         Max_MA 0.0156 MB         CA 2.0 MB         Max_CA 2.0 MB \n",
      "\n",
      "After placing the model on GPU:\n",
      "MA 6124.4683 MB         Max_MA 6124.4683 MB         CA 6140.0 MB         Max_CA 6140.0 MB \n",
      "\n",
      "Params (empirical) 6124.4526 MB\n",
      "\n",
      "Params (analytical, torch) 5941.8152 MB\n",
      "\n",
      "After input batch generation, before forward pass:\n",
      "MA 6124.4683 MB         Max_MA 6124.4683 MB         CA 6140.0 MB         Max_CA 6140.0 MB \n",
      "\n",
      "After backward:\n",
      "MA 54503.8359 MB         Max_MA 54503.8359 MB         CA 56948.0 MB         Max_CA 56948.0 MB \n",
      "\n",
      "\n",
      "Activations (analytical, torchviz) 50729.6655 MB\n",
      "Activations (empirical) 48379.3677 MB\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(RANDOM_SEED)\n",
    "configuration = GPT2Config(n_head=25, n_layer=48, n_positions=1024, n_embd=1600)\n",
    "model = GPT2Model(configuration)\n",
    "b = 2\n",
    "seq = configuration.n_positions\n",
    "x = torch.randint(0, configuration.vocab_size, size=(b,seq), device='cuda')\n",
    "estimate_layer_memory(copy.deepcopy(model), x, device='cuda', input_shape=None)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95606bd5-c4fc-4566-bd72-6fd7a388ea16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Before placing the model on GPU\n",
      "MA 0.0312 MB         Max_MA 0.0312 MB         CA 2.0 MB         Max_CA 2.0 MB \n",
      "\n",
      "After placing the model on GPU:\n",
      "MA 6124.4839 MB         Max_MA 6124.4839 MB         CA 6140.0 MB         Max_CA 6140.0 MB \n",
      "\n",
      "Params (empirical) 6124.4526 MB\n",
      "\n",
      "Params (analytical, torch) 5941.8152 MB\n",
      "\n",
      "After input batch generation, before forward pass:\n",
      "MA 6124.4839 MB         Max_MA 6124.4839 MB         CA 6140.0 MB         Max_CA 6140.0 MB \n",
      "\n",
      "After backward:\n",
      "MA 67049.7734 MB         Max_MA 67049.7734 MB         CA 67132.0 MB         Max_CA 67132.0 MB \n",
      "\n",
      "\n",
      "Activations (analytical, torchviz) 63059.322 MB\n",
      "Activations (empirical) 60925.2896 MB\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(RANDOM_SEED)\n",
    "configuration = GPT2Config(n_head=25, n_layer=48, n_positions=1024, n_embd=1600)\n",
    "configuration.use_dropmatmul = True\n",
    "configuration.use_lightsoftmax = True\n",
    "model = LightGPT2Model(configuration)\n",
    "b = 4\n",
    "seq = configuration.n_positions\n",
    "x = torch.randint(0, configuration.vocab_size, size=(b,seq), device='cuda')\n",
    "estimate_layer_memory(copy.deepcopy(model), x, device='cuda', input_shape=None)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0c813b-ebbd-4197-aa91-58c48e9f4340",
   "metadata": {},
   "source": [
    "# Correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da2dbeb1-507a-400e-a6b5-516084755fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(RANDOM_SEED)\n",
    "b = 2\n",
    "seq = 1024\n",
    "x1 = torch.randint(0, 50257, size=(b,seq), device='cuda')\n",
    "configuration = GPT2Config(n_layer=1)\n",
    "model1 = GPT2Model(configuration)\n",
    "model1 = model1.cuda()\n",
    "y1 = model1(x1).last_hidden_state\n",
    "y1.cos().mean().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c301c1a-3168-4237-8e2b-bc41b8dc3aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(RANDOM_SEED)\n",
    "b = 2\n",
    "seq = 1024\n",
    "x2 = torch.randint(0, 50257, size=(b,seq), device='cuda')\n",
    "configuration = GPT2Config(n_layer=1)\n",
    "configuration.use_dropmatmul = True\n",
    "configuration.use_lightsoftmax = True\n",
    "model2 = LightGPT2Model(configuration)\n",
    "model2 = model2.cuda()\n",
    "y2 = model2(x2).last_hidden_state\n",
    "y2.cos().mean().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccf6cb7b-82e2-464a-b185-43400061d0cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True, True, True, True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(x1, x2), \\\n",
    "torch.allclose(model1.h[0].attn.c_attn.weight, model2.h[0].attn.c_attn.weight), \\\n",
    "torch.allclose(y1, y2), \\\n",
    "torch.allclose(model1.h[0].attn.c_attn.weight.grad, model2.h[0].attn.c_attn.weight.grad), \\\n",
    "torch.allclose(model1.h[0].mlp.c_fc.weight.grad, model2.h[0].mlp.c_fc.weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8e69f5a-25ed-4275-acb5-428ad9e77dd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.5888e-06,  2.3713e-06,  1.1521e-06,  ...,  9.5301e-06,\n",
       "          -3.9004e-06,  3.4029e-06],\n",
       "         [ 2.2943e-06,  4.1748e-07, -1.6698e-06,  ...,  4.8337e-06,\n",
       "          -1.2031e-06, -1.3247e-05],\n",
       "         [-2.0791e-06, -5.0380e-07, -7.1940e-07,  ..., -1.0359e-05,\n",
       "          -8.5861e-06, -8.6150e-06],\n",
       "         ...,\n",
       "         [ 3.3493e-06,  2.2633e-06,  1.2692e-07,  ..., -3.9649e-06,\n",
       "           4.8613e-06,  5.5402e-06],\n",
       "         [-3.0494e-06,  8.7274e-07, -8.1078e-07,  ...,  4.1371e-06,\n",
       "          -1.0492e-05, -1.2938e-07],\n",
       "         [-1.7868e-06,  1.8953e-06, -2.4562e-06,  ...,  1.7462e-05,\n",
       "           6.1537e-06,  2.0687e-05]], device='cuda:0'),\n",
       " tensor([[ 1.5888e-06,  2.3713e-06,  1.1521e-06,  ...,  9.5301e-06,\n",
       "          -3.9004e-06,  3.4029e-06],\n",
       "         [ 2.2943e-06,  4.1749e-07, -1.6698e-06,  ...,  4.8337e-06,\n",
       "          -1.2031e-06, -1.3247e-05],\n",
       "         [-2.0791e-06, -5.0379e-07, -7.1940e-07,  ..., -1.0359e-05,\n",
       "          -8.5861e-06, -8.6150e-06],\n",
       "         ...,\n",
       "         [ 3.3493e-06,  2.2633e-06,  1.2691e-07,  ..., -3.9649e-06,\n",
       "           4.8613e-06,  5.5402e-06],\n",
       "         [-3.0494e-06,  8.7273e-07, -8.1079e-07,  ...,  4.1371e-06,\n",
       "          -1.0492e-05, -1.2938e-07],\n",
       "         [-1.7868e-06,  1.8953e-06, -2.4562e-06,  ...,  1.7462e-05,\n",
       "           6.1537e-06,  2.0687e-05]], device='cuda:0'))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.h[0].attn.c_attn.weight.grad, model2.h[0].attn.c_attn.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef9ffd3-0157-4323-8dce-81f46cf908aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
