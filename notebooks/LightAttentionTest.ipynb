{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c82a2675-0084-4d0e-93f1-8088e1ba7eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c360c0ea-3c63-42b0-ad48-aad81409834c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + '/trinity/home/d.cherniuk/libs/graphviz-2.50.0/bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b75face7-3ef0-4454-a251-a7b09098b236",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2Config\n",
    "import torch\n",
    "from torch import nn\n",
    "import copy\n",
    "import time\n",
    "\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Attention\n",
    "from light_attention.attention import LightAttention\n",
    "from light_attention.profile import estimate_layer_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ce8a812-3af0-4fe7-a7ca-19213c14e68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a866c40e-e816-4374-b86a-422159fc2c7b",
   "metadata": {},
   "source": [
    "# HuggingFace GPT2Attention vs Custom LightAttention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20180b88-724c-4e9c-a878-148e900178af",
   "metadata": {},
   "source": [
    "## Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd508ea4-8b63-4bcc-958b-6506b2cdadb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Before placing the model on GPU\n",
      "MA 0.0 MB         Max_MA 0.0 MB         CA 0.0 MB         Max_CA 0.0 MB \n",
      "\n",
      "After placing the model on GPU:\n",
      "MA 10.0122 MB         Max_MA 10.0122 MB         CA 22.0 MB         Max_CA 22.0 MB \n",
      "\n",
      "Params (empirical) 10.0122 MB\n",
      "\n",
      "Params (analytical, torch) 9.0117 MB\n",
      "\n",
      "After input batch generation, before forward pass:\n",
      "MA 40.0122 MB         Max_MA 40.0122 MB         CA 52.0 MB         Max_CA 52.0 MB \n",
      "\n",
      "Graph has been saved in huggingface_graph.pdf.\n",
      "\n",
      "After backward:\n",
      "MA 1721.0127 MB         Max_MA 1721.0127 MB         CA 1854.0 MB         Max_CA 1854.0 MB \n",
      "\n",
      "\n",
      "Activations (analytical, torchviz) 1830.0 MB\n",
      "Activations (empirical) 1711.0005 MB\n"
     ]
    }
   ],
   "source": [
    "configuration = GPT2Config(n_layer=1, embd_pdrop=0, attn_pdrop=0.1, resid_pdrop=0, summary_first_dropout=0, initializer_range=0.1)\n",
    "attn = GPT2Attention(configuration)\n",
    "\n",
    "b = 10\n",
    "seq = 1024\n",
    "emb = 768\n",
    "input_shape = (b,seq,emb)\n",
    "estimate_layer_memory(copy.deepcopy(attn), 'cuda', (b, seq, emb), fout='huggingface_graph', verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7279d818-f5a5-4e7f-b429-213e244b1cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Before placing the model on GPU\n",
      "MA 0.0 MB         Max_MA 0.0 MB         CA 0.0 MB         Max_CA 0.0 MB \n",
      "\n",
      "After placing the model on GPU:\n",
      "MA 10.0122 MB         Max_MA 10.0122 MB         CA 22.0 MB         Max_CA 22.0 MB \n",
      "\n",
      "Params (empirical) 10.0122 MB\n",
      "\n",
      "Params (analytical, torch) 9.0117 MB\n",
      "\n",
      "After input batch generation, before forward pass:\n",
      "MA 40.0122 MB         Max_MA 40.0122 MB         CA 52.0 MB         Max_CA 52.0 MB \n",
      "\n",
      "Graph has been saved in light_graph.pdf.\n",
      "\n",
      "After backward:\n",
      "MA 1301.0127 MB         Max_MA 1301.0127 MB         CA 1824.0 MB         Max_CA 1824.0 MB \n",
      "\n",
      "\n",
      "Activations (analytical, torchviz) 1350.0 MB\n",
      "Activations (empirical) 1291.0005 MB\n"
     ]
    }
   ],
   "source": [
    "configuration = GPT2Config(n_layer=1, embd_pdrop=0, attn_pdrop=0.1, resid_pdrop=0, summary_first_dropout=0, initializer_range=0.1)\n",
    "configuration.use_dropmatmul = True\n",
    "configuration.use_lightsoftmax = False\n",
    "attn = LightAttention(configuration)\n",
    "\n",
    "b = 10\n",
    "seq = 1024\n",
    "emb = 768\n",
    "input_shape = (b,seq,emb)\n",
    "estimate_layer_memory(copy.deepcopy(attn), 'cuda', (b, seq, emb), fout='light_graph', verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b4dd18-d21c-4945-b9a1-221e5ab4ddc1",
   "metadata": {},
   "source": [
    "## Correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c6d6264-2c45-4f5a-8952-6a5deef5f6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "configuration = GPT2Config(n_layer=1, embd_pdrop=0, attn_pdrop=0.1, resid_pdrop=0)\n",
    "attn1 = GPT2Attention(configuration).cuda()\n",
    "\n",
    "b = 10\n",
    "seq = 1024\n",
    "emb = 768\n",
    "x1 = torch.randn((b,seq,emb), dtype=torch.float, device='cuda')\n",
    "x1 = nn.Parameter(x1)\n",
    "\n",
    "y1 = attn1(x1)[0]\n",
    "\n",
    "loss1 = y1.mean().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc11fc5c-a2b2-4585-992e-6d43bf354586",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "configuration = GPT2Config(n_layer=1, embd_pdrop=0, attn_pdrop=0.1, resid_pdrop=0)\n",
    "configuration.use_dropmatmul = True\n",
    "configuration.use_lightsoftmax = True\n",
    "attn2 = LightAttention(configuration).cuda()\n",
    "\n",
    "b = 10\n",
    "seq = 1024\n",
    "emb = 768\n",
    "x2 = torch.randn((b,seq,emb), dtype=torch.float, device='cuda')\n",
    "x2 = nn.Parameter(x2)\n",
    "\n",
    "y2 = attn2(x2)[0]\n",
    "\n",
    "loss2 = y2.mean().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8373644-32c3-4a74-a470-1955a7bbcbae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True, True, True, True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(x1, x2), \\\n",
    "torch.allclose(attn1.c_attn.weight, attn2.c_attn.weight), \\\n",
    "torch.allclose(y1, y2), \\\n",
    "torch.allclose(attn1.c_attn.weight.grad, attn2.c_attn.weight.grad), \\\n",
    "torch.allclose(x1.grad, x2.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e5b849-20dc-421e-8988-6e1c260dfd20",
   "metadata": {},
   "source": [
    "# Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a598364-a993-4f70-af20-a8d9dbd00e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backward-forward loop takes 0.0239 seconds on average. Computed for 100 samples.\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "configuration = GPT2Config(n_layer=1, embd_pdrop=0, attn_pdrop=0.1, resid_pdrop=0)\n",
    "attn1 = GPT2Attention(configuration).cuda()\n",
    "\n",
    "samples = 100\n",
    "b = 10\n",
    "seq = 1024\n",
    "emb = 768\n",
    "\n",
    "# to mitigate the time needed for sample generation\n",
    "xs = torch.randn((samples, b,seq,emb), dtype=torch.float, device='cuda')\n",
    "\n",
    "# time1 = time.time()\n",
    "# with torch.no_grad():\n",
    "#     for x in xs: \n",
    "#         y = attn1(x)[0]    \n",
    "# time2 = time.time()\n",
    "# print(f'Forward pass takes {((time2-time1) / samples):.3} seconds on average. Computed for {samples} samples.')\n",
    "\n",
    "time1 = time.time()\n",
    "for x in xs: \n",
    "    y = attn1(x)[0]\n",
    "    y.mean().backward()\n",
    "time2 = time.time()\n",
    "\n",
    "print(f'Backward-forward loop takes {((time2-time1) / samples):.3} seconds on average. Computed for {samples} samples.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2012d88-a1fb-4266-86a9-6bc0378d8da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backward-forward loop takes 0.03 seconds on average. Computed for 100 samples.\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "configuration = GPT2Config(n_layer=1, embd_pdrop=0, attn_pdrop=0.1, resid_pdrop=0)\n",
    "configuration.use_dropmatmul = True\n",
    "configuration.use_lightsoftmax = True\n",
    "attn2 = LightAttention(configuration).cuda()\n",
    "\n",
    "samples = 100\n",
    "b = 10\n",
    "seq = 1024\n",
    "emb = 768\n",
    "\n",
    "# to mitigate the time needed for sample generation\n",
    "xs = torch.randn((samples, b,seq,emb), dtype=torch.float, device='cuda')\n",
    "\n",
    "# time1 = time.time()\n",
    "# with torch.no_grad():\n",
    "#     for x in xs: \n",
    "#         y = attn2(x)[0]    \n",
    "# time2 = time.time()\n",
    "# print(f'Forward pass takes {((time2-time1) / samples):.3} seconds on average. Computed for {samples} samples.')\n",
    "\n",
    "time1 = time.time()\n",
    "for x in xs: \n",
    "    y = attn2(x)[0]\n",
    "    y.mean().backward()\n",
    "time2 = time.time()\n",
    "\n",
    "print(f'Backward-forward loop takes {((time2-time1) / samples):.3} seconds on average. Computed for {samples} samples.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1682342-5545-4730-a38e-bbf4813504bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mark19",
   "language": "python",
   "name": "mark19"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
