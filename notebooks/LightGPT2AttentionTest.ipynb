{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c82a2675-0084-4d0e-93f1-8088e1ba7eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13f2fe5d-f6c9-4a4a-8ab5-c3478abf476f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Oct 13 21:41:16 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:02:00.0 Off |                  N/A |\n",
      "| 27%   34C    P8    21W / 260W |      0MiB / 11019MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b75face7-3ef0-4454-a251-a7b09098b236",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2Config\n",
    "import torch\n",
    "from torch import nn\n",
    "import copy\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Attention\n",
    "from light_attention.models.gpt2 import LightGPT2Attention\n",
    "from light_attention.profile import estimate_layer_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ce8a812-3af0-4fe7-a7ca-19213c14e68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 3407"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a866c40e-e816-4374-b86a-422159fc2c7b",
   "metadata": {},
   "source": [
    "# HuggingFace GPT2Attention vs LightGPT2Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20180b88-724c-4e9c-a878-148e900178af",
   "metadata": {},
   "source": [
    "## Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd508ea4-8b63-4bcc-958b-6506b2cdadb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Before placing the model on GPU\n",
      "MA 0.0 MB         Max_MA 0.0 MB         CA 0.0 MB         Max_CA 0.0 MB \n",
      "\n",
      "After placing the model on GPU:\n",
      "MA 10.0122 MB         Max_MA 10.0122 MB         CA 22.0 MB         Max_CA 22.0 MB \n",
      "\n",
      "Params (empirical) 10.0122 MB\n",
      "\n",
      "After input batch generation, before forward pass:\n",
      "MA 40.0122 MB         Max_MA 40.0122 MB         CA 52.0 MB         Max_CA 52.0 MB \n",
      "\n",
      "Graph has been saved in huggingface_graph.pdf.\n",
      "\n",
      "After backward:\n",
      "MA 1751.0127 MB         Max_MA 1751.0127 MB         CA 1854.0 MB         Max_CA 1854.0 MB \n",
      "\n",
      "Activations (empirical) 1741.0005 MB\n"
     ]
    }
   ],
   "source": [
    "configuration = GPT2Config(n_layer=1, embd_pdrop=0, attn_pdrop=0.1, resid_pdrop=0, summary_first_dropout=0, initializer_range=0.1)\n",
    "attn = GPT2Attention(configuration)\n",
    "\n",
    "b = 10\n",
    "seq = 1024\n",
    "emb = 768\n",
    "input_shape = (b,seq,emb)\n",
    "# if you don't have graphviz installed - set fout to None to skip graph building and rendering\n",
    "estimate_layer_memory(copy.deepcopy(attn), device='cuda', input_shape=input_shape, fout='huggingface_graph', verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7279d818-f5a5-4e7f-b429-213e244b1cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Before placing the model on GPU\n",
      "MA 0.0 MB         Max_MA 0.0 MB         CA 0.0 MB         Max_CA 0.0 MB \n",
      "\n",
      "After placing the model on GPU:\n",
      "MA 10.0122 MB         Max_MA 10.0122 MB         CA 22.0 MB         Max_CA 22.0 MB \n",
      "\n",
      "Params (empirical) 10.0122 MB\n",
      "\n",
      "After input batch generation, before forward pass:\n",
      "MA 40.0122 MB         Max_MA 40.0122 MB         CA 52.0 MB         Max_CA 52.0 MB \n",
      "\n",
      "Graph has been saved in light_graph.pdf.\n",
      "\n",
      "After backward:\n",
      "MA 851.0127 MB         Max_MA 851.0127 MB         CA 1344.0 MB         Max_CA 1344.0 MB \n",
      "\n",
      "Activations (empirical) 841.0005 MB\n"
     ]
    }
   ],
   "source": [
    "configuration = GPT2Config(n_layer=1, embd_pdrop=0, attn_pdrop=0.1, resid_pdrop=0, summary_first_dropout=0, initializer_range=0.1)\n",
    "configuration.use_dropmatmul = True\n",
    "configuration.use_lightsoftmax = True\n",
    "attn = LightGPT2Attention(configuration)\n",
    "\n",
    "b = 10\n",
    "seq = 1024\n",
    "emb = 768\n",
    "input_shape = (b,seq,emb)\n",
    "# if you don't have graphviz installed - set fout to None to skip graph building and rendering\n",
    "estimate_layer_memory(copy.deepcopy(attn), device='cuda', input_shape=input_shape, fout='light_graph', verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b4dd18-d21c-4945-b9a1-221e5ab4ddc1",
   "metadata": {},
   "source": [
    "## Correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c6d6264-2c45-4f5a-8952-6a5deef5f6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "configuration = GPT2Config(n_layer=1, embd_pdrop=0, attn_pdrop=0.1, resid_pdrop=0)\n",
    "attn1 = GPT2Attention(configuration).cuda()\n",
    "\n",
    "b = 10\n",
    "seq = 1024\n",
    "emb = 768\n",
    "x1 = torch.randn((b,seq,emb), dtype=torch.float, device='cuda')\n",
    "x1 = nn.Parameter(x1)\n",
    "\n",
    "y1 = attn1(x1)[0]\n",
    "\n",
    "y1.cos().mean().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67beafab-030a-433f-84cd-6643ae64577e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Oct 13 21:41:34 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:02:00.0 Off |                  N/A |\n",
      "| 27%   38C    P2    97W / 260W |   3654MiB / 11019MiB |      1%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   4145608      C   .../.conda/mark21/bin/python     3651MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc11fc5c-a2b2-4585-992e-6d43bf354586",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "configuration = GPT2Config(n_layer=1, embd_pdrop=0, attn_pdrop=0.1, resid_pdrop=0)\n",
    "configuration.use_dropmatmul = True\n",
    "configuration.use_lightsoftmax = True\n",
    "attn2 = LightGPT2Attention(configuration).cuda()\n",
    "\n",
    "b = 10\n",
    "seq = 1024\n",
    "emb = 768\n",
    "x2 = torch.randn((b,seq,emb), dtype=torch.float, device='cuda')\n",
    "x2 = nn.Parameter(x2)\n",
    "\n",
    "y2 = attn2(x2)[0]\n",
    "\n",
    "y2.cos().mean().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8373644-32c3-4a74-a470-1955a7bbcbae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True, False, False, False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(x1, x2), \\\n",
    "torch.allclose(attn1.c_attn.weight, attn2.c_attn.weight), \\\n",
    "torch.allclose(y1, y2), \\\n",
    "torch.allclose(attn1.c_attn.weight.grad, attn2.c_attn.weight.grad), \\\n",
    "torch.allclose(x1.grad, x2.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7803037-ee2e-41ea-821e-a2074c56b0fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-5.4295e-09,  3.3732e-08,  1.2488e-08,  ..., -5.4792e-07,\n",
       "          -4.2262e-08,  8.8687e-08],\n",
       "         [-9.5828e-09,  5.4152e-08, -4.3143e-09,  ..., -1.7547e-07,\n",
       "           9.1430e-08, -1.7308e-07],\n",
       "         [ 5.6753e-08,  4.4316e-08,  1.2221e-08,  ..., -2.4896e-07,\n",
       "           1.4978e-07, -1.4345e-07],\n",
       "         ...,\n",
       "         [-6.6360e-09, -2.7853e-09,  4.5166e-08,  ...,  3.2868e-07,\n",
       "          -2.6284e-07,  8.2507e-08],\n",
       "         [ 2.0470e-08,  9.8933e-09,  3.8972e-08,  ..., -5.0272e-07,\n",
       "          -1.0428e-07,  2.8344e-07],\n",
       "         [-4.0574e-08,  5.9461e-08,  2.4368e-08,  ...,  5.9955e-07,\n",
       "           4.4250e-07,  1.9155e-07]], device='cuda:0'),\n",
       " tensor([[-5.4295e-09,  3.3732e-08,  1.2488e-08,  ..., -5.4792e-07,\n",
       "          -4.2262e-08,  8.8687e-08],\n",
       "         [-9.5828e-09,  5.4152e-08, -4.3143e-09,  ..., -1.7547e-07,\n",
       "           9.1430e-08, -1.7308e-07],\n",
       "         [ 5.6753e-08,  4.4315e-08,  1.2221e-08,  ..., -2.4896e-07,\n",
       "           1.4978e-07, -1.4345e-07],\n",
       "         ...,\n",
       "         [-6.6360e-09, -2.7853e-09,  4.5166e-08,  ...,  3.2868e-07,\n",
       "          -2.6284e-07,  8.2507e-08],\n",
       "         [ 2.0470e-08,  9.8933e-09,  3.8972e-08,  ..., -5.0272e-07,\n",
       "          -1.0428e-07,  2.8344e-07],\n",
       "         [-4.0574e-08,  5.9461e-08,  2.4368e-08,  ...,  5.9955e-07,\n",
       "           4.4250e-07,  1.9155e-07]], device='cuda:0'))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn1.c_attn.weight.grad, attn2.c_attn.weight.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e5b849-20dc-421e-8988-6e1c260dfd20",
   "metadata": {},
   "source": [
    "## Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d10b7c7-d44b-49cf-a13a-a45af0f9a6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e8c59e-5cf7-460e-ac00-fea80cbd7fc4",
   "metadata": {},
   "source": [
    "### Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1e1f76f-2d93-4c3b-8f97-f86996d534b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 61.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass takes 0.0162 seconds on average. Computed for 100 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "configuration = GPT2Config(n_layer=1, embd_pdrop=0, attn_pdrop=0.1, resid_pdrop=0)\n",
    "attn1 = GPT2Attention(configuration).to(device)\n",
    "# attn1.eval()\n",
    "\n",
    "samples = 100\n",
    "b = 10\n",
    "seq = 1024\n",
    "emb = 768\n",
    "\n",
    "# to mitigate the time needed for sample generation\n",
    "xs = torch.randn((samples, b,seq,emb), dtype=torch.float, device=device)\n",
    "\n",
    "# fake run to allocate memory\n",
    "with torch.no_grad():\n",
    "    for x in tqdm(xs): \n",
    "        y = attn1(x)[0]\n",
    "        # cuda operations are asynchronous\n",
    "        torch.cuda.synchronize(device)\n",
    "        break\n",
    "\n",
    "time1 = time.time()\n",
    "with torch.no_grad():\n",
    "    for x in tqdm(xs): \n",
    "        y = attn1(x)[0]\n",
    "        # cuda operations are asynchronous\n",
    "        torch.cuda.synchronize(device)\n",
    "time2 = time.time()\n",
    "print(f'Forward pass takes {((time2-time1) / samples):.3} seconds on average. Computed for {samples} samples.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa5053a0-b667-4227-b53a-b0ae041b852a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Oct 13 20:41:22 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:02:00.0 Off |                  N/A |\n",
      "| 35%   47C    P2    94W / 260W |   8656MiB / 11019MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   4137421      C   .../.conda/mark21/bin/python     8653MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5bae479c-55f3-403c-a8d3-e451459d7906",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 61.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass takes 0.0163 seconds on average. Computed for 100 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "configuration = GPT2Config(n_layer=1, embd_pdrop=0, attn_pdrop=0.1, resid_pdrop=0)\n",
    "configuration.use_dropmatmul = True\n",
    "configuration.use_lightsoftmax = False\n",
    "attn2 = LightGPT2Attention(configuration).to(device)\n",
    "# attn2.eval()\n",
    "\n",
    "samples = 100\n",
    "b = 10\n",
    "seq = 1024\n",
    "emb = 768\n",
    "\n",
    "# to mitigate the time needed for sample generation\n",
    "xs = torch.randn((samples, b,seq,emb), dtype=torch.float, device=device)\n",
    "\n",
    "# fake run to allocate memory\n",
    "with torch.no_grad():\n",
    "    for x in tqdm(xs): \n",
    "        y = attn2(x)[0]\n",
    "        # cuda operations are asynchronous\n",
    "        torch.cuda.synchronize(device)\n",
    "        break\n",
    "\n",
    "time1 = time.time()\n",
    "with torch.no_grad():\n",
    "    for x in tqdm(xs): \n",
    "        y = attn2(x)[0]\n",
    "        # cuda operations are asynchronous\n",
    "        torch.cuda.synchronize(device)\n",
    "time2 = time.time()\n",
    "print(f'Forward pass takes {((time2-time1) / samples):.3} seconds on average. Computed for {samples} samples.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ff76fb-47f8-47ff-9fbd-875e8bf31c02",
   "metadata": {},
   "source": [
    "### Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df87f8f3-7877-41a1-abda-f834f0e743cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:04<00:00, 23.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backward-forward loop takes 0.0419 seconds on average. Computed for 100 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "configuration = GPT2Config(n_layer=1, embd_pdrop=0, attn_pdrop=0.1, resid_pdrop=0)\n",
    "attn1 = GPT2Attention(configuration).to(device)\n",
    "\n",
    "samples = 100\n",
    "b = 10\n",
    "seq = 1024\n",
    "emb = 768\n",
    "\n",
    "# to mitigate the time needed for sample generation\n",
    "xs = torch.randn((samples, b,seq,emb), dtype=torch.float, device=device)\n",
    "\n",
    "# fake run to allocate memory\n",
    "if device == 'cuda':\n",
    "    for x in tqdm(xs): \n",
    "        y = attn1(x)[0]\n",
    "        y.mean().backward()\n",
    "        # cuda operations are asynchronous\n",
    "        torch.cuda.synchronize(device)\n",
    "        break\n",
    "\n",
    "time1 = time.time()\n",
    "for x in tqdm(xs): \n",
    "    y = attn1(x)[0]\n",
    "    y.mean().backward()\n",
    "    torch.cuda.synchronize(device)\n",
    "time2 = time.time()\n",
    "print(f'Backward-forward loop takes {((time2-time1) / samples):.3} seconds on average. Computed for {samples} samples.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2012d88-a1fb-4266-86a9-6bc0378d8da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:04<00:00, 20.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backward-forward loop takes 0.0478 seconds on average. Computed for 100 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "configuration = GPT2Config(n_layer=1, embd_pdrop=0, attn_pdrop=0.1, resid_pdrop=0)\n",
    "configuration.use_dropmatmul = True\n",
    "configuration.use_lightsoftmax = False\n",
    "attn2 = LightGPT2Attention(configuration).to(device)\n",
    "\n",
    "samples = 100\n",
    "b = 10\n",
    "seq = 1024\n",
    "emb = 768\n",
    "\n",
    "# to mitigate the time needed for sample generation\n",
    "xs = torch.randn((samples, b,seq,emb), dtype=torch.float, device=device)\n",
    "\n",
    "# fake run to allocate memory\n",
    "if device == 'cuda':\n",
    "    for x in tqdm(xs): \n",
    "        y = attn2(x)[0]\n",
    "        y.mean().backward()\n",
    "        # cuda operations are asynchronous\n",
    "        torch.cuda.synchronize(device)\n",
    "        break\n",
    "\n",
    "time1 = time.time()\n",
    "for x in tqdm(xs): \n",
    "    y = attn2(x)[0]\n",
    "    y.mean().backward()\n",
    "    # cuda operations are asynchronous\n",
    "    torch.cuda.synchronize(device)\n",
    "time2 = time.time()\n",
    "print(f'Backward-forward loop takes {((time2-time1) / samples):.3} seconds on average. Computed for {samples} samples.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23145c86-42fe-4439-9eac-943c199c44ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mark21",
   "language": "python",
   "name": "mark21"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
